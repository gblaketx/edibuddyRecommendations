{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pickle\n",
    "import DataLoader\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings = pd.read_csv(\"data/all_recipe_clean.csv\")\n",
    "with open(\"data/test_data_100.pkl\", \"rb\") as infile:\n",
    "    recipes = pickle.load(infile) \n",
    "ratings = pd.read_csv(\"data/test_ratings_100.csv\")\n",
    "dataLoader = DataLoader.DataLoader(ratings, recipes)\n",
    "user_holdout, recipe_holdout, holdout = dataLoader.get_holdout_data()\n",
    "holdout_X = [t[:2] for t in holdout]\n",
    "holdout_Y = np.array([t[2] for t in holdout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeMeanEstimator:\n",
    "    \"\"\"\n",
    "    Just guesses the mean for the recipe\n",
    "    \"\"\"\n",
    "    def __init__(self, dataLoader, recipe_holdout):\n",
    "        self.dataLoader = dataLoader\n",
    "        self.recipe_holdout = recipe_holdout\n",
    "        \n",
    "    def get_recipe_average(self, recipe_id):\n",
    "        ratings = self.dataLoader.get_recipe_ratings(\n",
    "            recipe_id, self.recipe_holdout)\n",
    "        return np.round(np.mean(np.fromiter(ratings.values(), dtype=np.int64)))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: list of (user_id, recipe_id) tuple\n",
    "        y: np.array user rating\n",
    "        \"\"\"\n",
    "        return self.predict(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: list of (user_id, recipe_id) tuple\n",
    "        \"\"\"\n",
    "        return np.array([self.get_recipe_average(recipe_id) for _, recipe_id in X])\n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return {\n",
    "            'dataLoader': self.dataLoader,\n",
    "            'recipe_holdout': self.recipe_holdout\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserMeanEstimator:\n",
    "    \"\"\"\n",
    "    Just guesses the user rating mean\n",
    "    \"\"\"\n",
    "    def __init__(self, dataLoader, user_holdout):\n",
    "        self.dataLoader = dataLoader\n",
    "        self.user_holdout = user_holdout\n",
    "        \n",
    "    def get_user_average(self, user_id):\n",
    "        ratings = self.dataLoader.get_user_ratings(user_id, self.user_holdout)\n",
    "        return np.round(np.mean(np.fromiter(ratings.values(), dtype=np.int64)))\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: list of (user_id, recipe_id) tuple\n",
    "        y: np.array user rating\n",
    "        \"\"\"\n",
    "        return self.predict(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: list of (user_id, recipe_id) tuple\n",
    "        \"\"\"\n",
    "        return np.array([self.get_user_average(user_id) for user_id, _ in X])\n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return {\n",
    "            'dataLoader': self.dataLoader,\n",
    "            'user_holdout': self.user_holdout\n",
    "        }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TunedMeanEstimator:\n",
    "    \"\"\"\n",
    "    Takes a weighted average of the user's average rating and the\n",
    "    recipe's average rating, then tunes the weighting parameter w\n",
    "    based on data.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataLoader, recipe_holdout, user_holdout):\n",
    "        self.dataLoader = dataLoader\n",
    "        self.recipe_holdout = recipe_holdout\n",
    "        self.user_holdout = user_holdout\n",
    "        self.w = np.array([0.5, 0.5])\n",
    "        self.eta = 0.01\n",
    "#         self.classifier = SGDClassifier()\n",
    "#         self.vec = DictVectorizer()\n",
    "  \n",
    "    def get_recipe_average(self, recipe_id):\n",
    "        ratings = self.dataLoader.get_recipe_ratings(\n",
    "            recipe_id, self.recipe_holdout)\n",
    "        return np.mean(np.fromiter(ratings.values(), dtype=np.int64))\n",
    "    \n",
    "    def get_user_average(self, user_id):\n",
    "        ratings = self.dataLoader.get_user_ratings(user_id, self.user_holdout)\n",
    "        return np.mean(np.fromiter(ratings.values(), dtype=np.int64))\n",
    "        \n",
    "#     def get_feat_dict(self, user_id, recipe_id):\n",
    "#         return {\n",
    "#             \"user_avg\": self.get_user_average(user_id),\n",
    "#             \"recipe_avg\": self.get_recipe_average(recipe_id),\n",
    "#         }\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.eta = 0.01\n",
    "        feats = [np.array([self.get_user_average(user_id), self.get_recipe_average(recipe_id)]) for user_id, recipe_id in X]\n",
    "#         feats = [self.get_feat_dict(user_id, recipe_id) for user_id, recipe_id in X]\n",
    "#         feats = self.vec.fit_transform(feats)\n",
    "#         return self.classifier.fit(feats, y)\n",
    "        for epoch in range(10):\n",
    "            for i, feat in enumerate(feats):\n",
    "                pred = np.dot(self.w, feat)\n",
    "#                 print(self.w, feat, pred, y[i])\n",
    "                loss = pred - y[i]\n",
    "                self.w -= self.eta*2*loss*feat\n",
    "            self.eta /= 2\n",
    "        print(self.w)\n",
    "    \n",
    "    def predict(self, X):\n",
    "#         feats = [self.get_feat_dict(user_id, recipe_id) for user_id, recipe_id in X]\n",
    "        feats = [np.array([self.get_user_average(user_id), self.get_recipe_average(recipe_id)]) for user_id, recipe_id in X]\n",
    "        return [np.round(np.dot(self.w, feat)) for feat in feats]\n",
    "#         feats = self.vec.transform(feats)\n",
    "#         return self.classifier.predict(feats)\n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return {\n",
    "            'dataLoader': self.dataLoader,\n",
    "            'user_holdout': self.user_holdout,\n",
    "            'recipe_holdout': self.recipe_holdout,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSGDEstimator:\n",
    "    \"\"\"\n",
    "    Takes a weighted average of the user's average rating and the\n",
    "    recipe's average rating, then tunes the weighting parameter w\n",
    "    based on data.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataLoader, recipe_holdout, user_holdout):\n",
    "        self.dataLoader = dataLoader\n",
    "        self.recipe_holdout = recipe_holdout\n",
    "        self.user_holdout = user_holdout\n",
    "        self.classifier = SGDClassifier()\n",
    "        self.vec = DictVectorizer()\n",
    "  \n",
    "    def get_recipe_average(self, recipe_id):\n",
    "        ratings = self.dataLoader.get_recipe_ratings(\n",
    "            recipe_id, self.recipe_holdout)\n",
    "        return np.mean(np.fromiter(ratings.values(), dtype=np.int64))\n",
    "    \n",
    "    def get_user_average(self, user_id):\n",
    "        ratings = self.dataLoader.get_user_ratings(user_id, self.user_holdout)\n",
    "        return np.mean(np.fromiter(ratings.values(), dtype=np.int64))\n",
    "        \n",
    "    def get_feat_dict(self, user_id, recipe_id):\n",
    "        return {\n",
    "            \"user_avg\": self.get_user_average(user_id),\n",
    "            \"recipe_avg\": self.get_recipe_average(recipe_id),\n",
    "        }\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        feats = [self.get_feat_dict(user_id, recipe_id) for user_id, recipe_id in X]\n",
    "        feats = self.vec.fit_transform(feats)\n",
    "        return self.classifier.fit(feats, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        feats = [self.get_feat_dict(user_id, recipe_id) for user_id, recipe_id in X]\n",
    "        feats = self.vec.transform(feats)\n",
    "        return self.classifier.predict(feats)\n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return {\n",
    "            'dataLoader': self.dataLoader,\n",
    "            'user_holdout': self.user_holdout,\n",
    "            'recipe_holdout': self.recipe_holdout,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFilteringEstimator:\n",
    "    \"\"\"\n",
    "    Uses item-item collaborative filtering to predict the user rating\n",
    "    \"\"\"\n",
    "    def __init__(self, dataLoader, recipe_holdout, user_holdout, similarity=None):\n",
    "        self.dataLoader = dataLoader\n",
    "        self.recipe_holdout = recipe_holdout\n",
    "        self.user_holdout = user_holdout\n",
    "        self.index_to_recipe_id = [id for id in self.dataLoader.get_recipe_ids()]\n",
    "        self.recipe_id_to_index = { id: idx for idx, id in enumerate(self.index_to_recipe_id) }\n",
    "        if similarity is None:\n",
    "            self.similarity = self.compute_similarity_matrix()\n",
    "        else:\n",
    "            self.similarity = similarity\n",
    "        \n",
    "    def compute_similarity_matrix(self):\n",
    "        \n",
    "        num_recipes = len(self.recipe_id_to_index)\n",
    "        sim = np.zeros((num_recipes, num_recipes))\n",
    "        \n",
    "        # since symmetric, only have sim[smallerIndex][largerIndex]\n",
    "        indices = list(enumerate(self.index_to_recipe_id))\n",
    "        \n",
    "        for idx1, recipe1 in indices:\n",
    "            ratings1 = self.dataLoader.get_recipe_ratings(recipe1, self.recipe_holdout)\n",
    "            \n",
    "            related_recipes = set()\n",
    "            for user_id in dataLoader.get_recipe_ratings(5, recipe_holdout).keys():\n",
    "                rated_recipes = dataLoader.get_user_ratings(user_id, user_holdout).keys()\n",
    "                related_recipes = related_recipes.union(rated_recipes)\n",
    "            \n",
    "            if idx1 % 100 == 0:\n",
    "                print(\"Similarity computation progress\", idx1)\n",
    "            for recipe2 in related_recipes:\n",
    "                idx2 = self.recipe_id_to_index[recipe2]\n",
    "                if idx2 > idx1:\n",
    "                    sim[idx1][idx2] = self.compute_cosine_similiarity(ratings1, recipe2)\n",
    "        return sim\n",
    "    \n",
    "    def compute_cosine_similiarity(self, ratings1, r2):\n",
    "        ratings2 = self.dataLoader.get_recipe_ratings(r2, self.recipe_holdout)\n",
    "        prod = 0\n",
    "        for k in ratings1:\n",
    "            if k in ratings2:\n",
    "                prod += ratings1[k] * ratings2[k]\n",
    "        return prod / (len(ratings1) * len(ratings2))\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: list of (user_id, recipe_id) tuple\n",
    "        y: np.array user rating\n",
    "        \"\"\"\n",
    "        return self.predict(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: list of (user_id, recipe_id) tuple\n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        for user_id, recipe_id in X:\n",
    "            user_ratings = self.dataLoader.get_user_ratings(user_id, self.user_holdout)\n",
    "            numer = 0.0\n",
    "            denom = 0.0\n",
    "            pred_idx = self.recipe_id_to_index[recipe_id]\n",
    "            for neighbor_rid, rating in user_ratings.items():\n",
    "                neighbor_idx = self.recipe_id_to_index[neighbor_rid]\n",
    "                low_id, high_id = tuple(sorted([pred_idx, neighbor_idx]))\n",
    "                sim = self.similarity[low_id][high_id]\n",
    "                numer += sim * rating\n",
    "                denom += sim\n",
    "            if denom == 0.0:\n",
    "                pred = np.mean(np.fromiter(user_ratings.values(), dtype=np.int64))\n",
    "            else:\n",
    "                pred = numer / denom\n",
    "            preds.append(pred)\n",
    "        return np.round(np.array(preds))\n",
    "        \n",
    "    def get_params(self, deep=False):\n",
    "        return {\n",
    "            'dataLoader': self.dataLoader,\n",
    "            'user_holdout': self.user_holdout,\n",
    "            'recipe_holdout': self.recipe_holdout,\n",
    "            'similarity': self.similarity,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticEstimator:\n",
    "    def __init__(self, dataLoader, recipe_holdout, user_holdout):\n",
    "        self.dataLoader = dataLoader\n",
    "        self.recipe_holdout = recipe_holdout\n",
    "        self.user_holdout = user_holdout\n",
    "        self.regressor = LogisticRegression(max_iter=100, penalty='l1')\n",
    "        self.vec = DictVectorizer()\n",
    "        \n",
    "        self.recipe_id_to_index = { id: idx for idx, id in enumerate(dataLoader.get_recipe_ids()) }\n",
    "        \n",
    "    def extract_features(self, X):\n",
    "        feats = [self.get_feature_dict(user_id, recipe_id) for user_id, recipe_id in X]\n",
    "#         f = self.vec.fit_transform(feats)\n",
    "        return feats\n",
    "\n",
    "#     def extract_features(self, X):\n",
    "#         recipe_ratings_matrix = sp.sparse.lil_matrix((len(X), len(self.recipe_id_to_index)), dtype=np.int8)\n",
    "#         feats = []\n",
    "#         for i, ids in enumerate(X):\n",
    "#             user_id, recipe_id = ids\n",
    "#             item_feats = self.get_feature_dict(user_id, recipe_id, i, recipe_ratings_matrix)\n",
    "#             feats.append(item_feats)\n",
    "#         f = self.vec.fit_transform(feats)\n",
    "#         f = sp.sparse.hstack([f, recipe_ratings_matrix])\n",
    "#         return f\n",
    "        \n",
    "    def get_feature_dict(self, user_id, recipe_id):\n",
    "        user_ratings = self.dataLoader.get_user_ratings(user_id, self.user_holdout)\n",
    "        avg_user_rating = np.mean(np.fromiter(user_ratings.values(), dtype=np.int64))\n",
    "        \n",
    "        recipe_ratings =  self.dataLoader.get_recipe_ratings(\n",
    "            recipe_id, self.recipe_holdout)\n",
    "        avg_recipe_rating = np.mean(np.fromiter(recipe_ratings.values(), dtype=np.int64))\n",
    "        recipe_info = self.dataLoader.get_recipe_info(recipe_id)\n",
    "        \n",
    "        feats = {\n",
    "            'user_id': user_id,\n",
    "            'recipe_id': recipe_id,\n",
    "            'avg_user_rating': avg_user_rating,\n",
    "            'avg_recipe_rating': avg_recipe_rating,\n",
    "            'calories': recipe_info['calories']\n",
    "        }\n",
    "        \n",
    "        for category in recipe_info[\"categories\"]:\n",
    "            feats[\"cat_{}\".format(category)] = 1\n",
    "        for ingredient in recipe_info[\"ingredients\"]:\n",
    "            ing_name = ingredient[\"key ingredient\"]\n",
    "            feats[\"ing_{}\".format(ing_name)] = ingredient[\"quantity\"]\n",
    "        \n",
    "#         for other_recipe_id, rating in user_ratings.items():\n",
    "#             feats[str((user_id, other_recipe_id))] = rating\n",
    "#         for other_user_id, rating in recipe_ratings.items():\n",
    "#             feats[str((other_user_id, recipe_id))] = rating\n",
    "        \n",
    "        return feats\n",
    "        \n",
    "    \n",
    "#     def get_feature_dict(self, user_id, recipe_id, i, recipe_mat):\n",
    "#         user_ratings = self.dataLoader.get_user_ratings(user_id, self.user_holdout)\n",
    "#         avg_user_rating = np.mean(np.fromiter(user_ratings.values(), dtype=np.int64))\n",
    "        \n",
    "#         recipe_ratings =  self.dataLoader.get_recipe_ratings(\n",
    "#             recipe_id, self.recipe_holdout)\n",
    "#         avg_recipe_rating = np.mean(np.fromiter(recipe_ratings.values(), dtype=np.int64))\n",
    "\n",
    "#         feats = {\n",
    "#             'user_id': user_id,\n",
    "#             'recipe_id': recipe_id,\n",
    "#             'avg_user_rating': avg_user_rating,\n",
    "#             'avg_recipe_rating': avg_recipe_rating,\n",
    "#         }\n",
    "        \n",
    "#         for other_recipe_id, rating in user_ratings.items():\n",
    "# #             feats[str((user_id, other_recipe_id))] = rating\n",
    "#             recipe_mat[i, self.recipe_id_to_index[other_recipe_id]] = rating\n",
    "# #         for other_user_id, rating in recipe_ratings.items():\n",
    "# #             feats[str((other_user_id, recipe_id))] = rating\n",
    "        \n",
    "#         return feats\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: list of (user_id, recipe_id) tuple\n",
    "        y: np.array user rating\n",
    "        \"\"\"\n",
    "        feats = self.extract_features(X)\n",
    "        feats = self.vec.fit_transform(feats)\n",
    "        return self.regressor.fit(feats, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: list of (user_id, recipe_id) tuple\n",
    "        \"\"\"\n",
    "        feats = self.extract_features(X)\n",
    "        feats = self.vec.transform(feats)\n",
    "        return self.regressor.predict(feats)\n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return {\n",
    "            'dataLoader': self.dataLoader,\n",
    "            'user_holdout': self.user_holdout,\n",
    "            'recipe_holdout': self.recipe_holdout,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestEstimator:\n",
    "    def __init__(self, dataLoader, recipe_holdout, user_holdout):\n",
    "        self.dataLoader = dataLoader\n",
    "        self.recipe_holdout = recipe_holdout\n",
    "        self.user_holdout = user_holdout\n",
    "        self.classifier = RandomForestClassifier(n_estimators=100)\n",
    "        self.vec = DictVectorizer()\n",
    "    \n",
    "    def extract_features(self, X):\n",
    "        feats = [self.get_feature_dict(user_id, recipe_id) for user_id, recipe_id in X]\n",
    "#         f = self.vec.fit_transform(feats)\n",
    "        return feats\n",
    "    \n",
    "    def get_feature_dict(self, user_id, recipe_id):\n",
    "        user_ratings = self.dataLoader.get_user_ratings(user_id, self.user_holdout)\n",
    "        avg_user_rating = np.mean(np.fromiter(user_ratings.values(), dtype=np.int64))\n",
    "        \n",
    "        recipe_ratings =  self.dataLoader.get_recipe_ratings(\n",
    "            recipe_id, self.recipe_holdout)\n",
    "        avg_recipe_rating = np.mean(np.fromiter(recipe_ratings.values(), dtype=np.int64))\n",
    "\n",
    "        feats = {\n",
    "            'user_id': user_id,\n",
    "            'recipe_id': recipe_id,\n",
    "            'avg_user_rating': avg_user_rating,\n",
    "            'avg_recipe_rating': avg_recipe_rating,\n",
    "        }\n",
    "        \n",
    "        for other_recipe_id, rating in user_ratings.items():\n",
    "            feats[str((user_id, other_recipe_id))] = rating\n",
    "        for other_user_id, rating in recipe_ratings.items():\n",
    "            feats[str((other_user_id, recipe_id))] = rating\n",
    "        \n",
    "        return feats\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: list of (user_id, recipe_id) tuple\n",
    "        y: np.array user rating\n",
    "        \"\"\"\n",
    "        feats = self.extract_features(X)\n",
    "        feats =  self.vec.fit_transform(feats)\n",
    "        return self.classifier.fit(feats, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: list of (user_id, recipe_id) tuple\n",
    "        \"\"\"\n",
    "        feats = self.extract_features(X)\n",
    "        feats = self.vec.transform(feats)\n",
    "        return self.classifier.predict(feats)\n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return {\n",
    "            'dataLoader': self.dataLoader,\n",
    "            'user_holdout': self.user_holdout,\n",
    "            'recipe_holdout': self.recipe_holdout,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity computation progress 0\n",
      "Similarity computation progress 100\n",
      "Similarity computation progress 200\n",
      "Similarity computation progress 300\n",
      "Similarity computation progress 400\n",
      "Similarity computation progress 500\n",
      "Similarity computation progress 600\n",
      "Similarity computation progress 700\n",
      "Similarity computation progress 800\n",
      "Similarity computation progress 900\n",
      "Similarity computation progress 1000\n",
      "Similarity computation progress 1100\n",
      "Similarity computation progress 1200\n",
      "Similarity computation progress 1300\n",
      "Similarity computation progress 1400\n",
      "Similarity computation progress 1500\n",
      "Similarity computation progress 1600\n",
      "Similarity computation progress 1700\n",
      "Similarity computation progress 1800\n",
      "Similarity computation progress 1900\n",
      "Similarity computation progress 2000\n",
      "Similarity computation progress 2100\n",
      "Similarity computation progress 2200\n",
      "Similarity computation progress 2300\n",
      "Similarity computation progress 2400\n",
      "Similarity computation progress 2500\n",
      "Similarity computation progress 2600\n",
      "Similarity computation progress 2700\n",
      "Similarity computation progress 2800\n",
      "Similarity computation progress 2900\n",
      "Similarity computation progress 3000\n",
      "Similarity computation progress 3100\n",
      "Similarity computation progress 3200\n"
     ]
    }
   ],
   "source": [
    "recipeMeanEstimator = RecipeMeanEstimator(dataLoader, recipe_holdout)\n",
    "# recipeMeanEstimator.fit(holdout_X, holdout_Y)\n",
    "\n",
    "userMeanEstimator = UserMeanEstimator(dataLoader, user_holdout)\n",
    "# userMeanEstimator.fit(holdout_X, holdout_Y)\n",
    "\n",
    "collabFilterEstimator = CollaborativeFilteringEstimator(dataLoader, recipe_holdout, user_holdout)\n",
    "\n",
    "logitEstimator = LogisticEstimator(dataLoader, recipe_holdout, user_holdout)\n",
    "# logitEstimator.fit(holdout_X, holdout_Y)\n",
    "\n",
    "# logitEstimator.predict(holdout_X[:1000])\n",
    "\n",
    "randomForestEstimator = RandomForestEstimator(dataLoader, recipe_holdout, user_holdout)\n",
    "# randomForestEstimator.fit(holdout_X, holdout_Y)\n",
    "# randomForestEstimator.predict(holdout_X[:1000])\n",
    "\n",
    "tunedMeanEstimator = TunedMeanEstimator(dataLoader, recipe_holdout, user_holdout)\n",
    "# tunedMeanEstimator.fit(holdout_X, holdout_Y)\n",
    "# tunedMeanEstimator.predict(holdout_X[:10])\n",
    "\n",
    "simpleSGDEstimator = SimpleSGDEstimator(dataLoader, recipe_holdout, user_holdout)\n",
    "\n",
    "\n",
    "# cross_val_score(tunedMeanEstimator, holdout_X, holdout_Y, cv=5, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe Mean Estimator Cross-Val F1-Macro Score 0.15476658380193647\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-c8b13f389d32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m print(\"Random Forest Cross-Val F1-Macro Score\",\n\u001b[1;32m---> 14\u001b[1;33m       np.mean(cross_val_score(randomForestEstimator, holdout_X, holdout_Y, cv=5, scoring='f1_macro')))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-0f99512a16d6>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    515\u001b[0m                                       \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                                       \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m                                       accept_large_sparse=accept_large_sparse)\n\u001b[0m\u001b[0;32m    518\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[1;31m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m             _assert_all_finite(spmatrix.data,\n\u001b[1;32m--> 350\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "print(\"Recipe Mean Estimator Cross-Val F1-Macro Score\",\n",
    "      np.mean(cross_val_score(recipeMeanEstimator, holdout_X, holdout_Y, cv=5, scoring='f1_macro')))\n",
    "\n",
    "print(\"User Mean Estimator Cross-Val F1-Macro Score\",\n",
    "      np.mean(cross_val_score(userMeanEstimator, holdout_X, holdout_Y, cv=5, scoring='f1_macro')))\n",
    " \n",
    "print(\"Collab Filter Estimator Cross-Val F1-Macro Score\",\n",
    "      np.mean(cross_val_score(collabFilterEstimator, holdout_X, holdout_Y, cv=5, scoring='f1_macro')))\n",
    "\n",
    "print(\"Logistic Regression Cross-Val F1-Macro Score\",\n",
    "     np.mean(cross_val_score(logitEstimator, holdout_X, holdout_Y, cv=5, scoring='f1_macro')))\n",
    "\n",
    "print(\"Random Forest Cross-Val F1-Macro Score\",\n",
    "      np.mean(cross_val_score(randomForestEstimator, holdout_X, holdout_Y, cv=5, scoring='f1_macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gblak\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.2308051 , 0.20650424, 0.18901866, 0.17943824, 0.23043864])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collabFilterEstimator.predict(holdout_X)\n",
    "# cross_val_score(recipeMeanEstimator, holdout_X, holdout_Y, cv=5, scoring='f1_macro')\n",
    "# cross_val_score(logitEstimator, holdout_X, holdout_Y, cv=5, scoring='f1_macro')\n",
    "# cross_val_score(collabFilterEstimator, holdout_X, holdout_Y, cv=5, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15837094399999999"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logitEstimator = LogisticEstimator(dataLoader, recipe_holdout, user_holdout)\n",
    "# logitEstimator.fit(holdout_X, holdout_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gblak\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08264884874505347"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleSGDEstimator = SimpleSGDEstimator(dataLoader, recipe_holdout, user_holdout)\n",
    "np.mean(cross_val_score(simpleSGDEstimator, holdout_X, holdout_Y, cv=5, scoring='f1_macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.mean(cross_val_score(tunedMeanEstimator, holdout_X, holdout_Y, cv=5, scoring='f1_macro'))\n",
    "logitEstimator = LogisticEstimator(dataLoader, recipe_holdout, user_holdout)\n",
    "logitEstimator.fit(holdout_X, holdout_Y)\n",
    "logitEstimator.predict(holdout_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
